{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionalLayer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO6E6ZqxKcoe7Ak4Av4KLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/NuralNetwork_scratch/blob/main/CNN/ConvolutionalLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IpwmtBke1F9"
      },
      "source": [
        "# conolutional layer\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUignDQe8Vv"
      },
      "source": [
        "class con2D:\r\n",
        "  def __init__(self, filter , kernal_size, img_channel,strides = (1,1) ):\r\n",
        "    self.filter = None\r\n",
        "    self.bias = None\r\n",
        "    self.input = None\r\n",
        "    \"\"\"Convolute the image with the filter and strids\"\"\"\r\n",
        "    \r\n",
        "    \r\n",
        "    f_channel = img_channel               #filter channel must match with the image channel\r\n",
        "    self.no_filters = filter                   # number of filters\r\n",
        "    f_dim , _ = kernal_size\r\n",
        "    self.f_dim =  f_dim              # get the filter size\r\n",
        "    self.strides , _ = strides\r\n",
        "    f_size = (self.no_filters , f_channel , f_dim , f_dim) # get the strides dimentions\r\n",
        "\r\n",
        "    #initialize the filter\r\n",
        "    \"\"\" Initalize the filter with the noraml distribution with the standard deviation inversely proportional the square root of the number of units\"\"\"\r\n",
        "    sdv = 1.0 / np.sqrt(np.prod(f_size))\r\n",
        "    self.filter = np.random.normal(loc = 0, scale = sdv, size = f_size)\r\n",
        "    #self.filter = np.array([[[1,0,1] , [2,1,1] , [0,1,2]]])\r\n",
        "    self.bias = np.zeros((self.no_filters , 1)) # each for the one filters    \r\n",
        "\r\n",
        "  \r\n",
        "  #forward propogaton\r\n",
        "  def forward_propogation(self, image):\r\n",
        "    self.input = image\r\n",
        "    (img_channel , input_dim , _ ) = image.shape #input dimensions channel first dimention \r\n",
        "    \r\n",
        "    #calculate the output dimension, so according to that i can create the empty array to store value\r\n",
        "    out_dim = int(((input_dim - self.f_dim) / self.strides) + 1)\r\n",
        "    print('out put dimention',out_dim)\r\n",
        "    #create the empty array with output dimetions\r\n",
        "    self.output = np.zeros((1 , out_dim, out_dim )) #syntax np.zeros(channel , row , col)\r\n",
        "    for curr_filter in range(self.no_filters): # for loop over the number of filters\r\n",
        "      curr_y = out_y =0\r\n",
        "      \r\n",
        "      #move vertically over the image\r\n",
        "      while (curr_y + self.f_dim) <= input_dim:\r\n",
        "        curr_x = out_x = 0\r\n",
        "        #move horizondally\r\n",
        "        while (curr_x + self.f_dim) <= input_dim:\r\n",
        "          \r\n",
        "          \"\"\"perform convolution operation on the image\"\"\"\r\n",
        "          self.output[curr_filter , curr_y , curr_x] = np.sum(self.filter[curr_filter] * image[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim]) #+ self.bias[curr_filter]\r\n",
        "\r\n",
        "          curr_x += self.strides\r\n",
        "          out_x +=1\r\n",
        "        curr_y += self.strides\r\n",
        "        out_y += 1\r\n",
        "    print(self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self , error_input , learning_rate):\r\n",
        "      \"\"\"update the filter \"\"\" # gonna use the self.no_filters and self.f_dim\r\n",
        "      image_channel , img_dim , _ = self.input.shape\r\n",
        "\r\n",
        "      #initialize the derivations by creating zero array\r\n",
        "      dout = np.zeros(self.input.shape)\r\n",
        "      d_filter = np.zeros(self.filter.shape)\r\n",
        "      d_bias = np.zeros(self.bias.shape)\r\n",
        "      for curr_filter in range(self.no_filters): # for loop over the number of filters\r\n",
        "        curr_y = out_y =0\r\n",
        "      \r\n",
        "        #move vertically over the image\r\n",
        "        while (curr_y + self.f_dim) <= img_dim:\r\n",
        "          curr_x = out_x = 0\r\n",
        "          #move horizondally\r\n",
        "          while (curr_x + self.f_dim) <= img_dim:\r\n",
        "          \r\n",
        "            \"\"\"loss gradiant on filters\"\"\"\r\n",
        "            #update the filter\r\n",
        "            print('filter' , self.filter)\r\n",
        "            print('zero filter' , d_filter)\r\n",
        "            print(d_filter[curr_filter])\r\n",
        "            print(error_input[curr_filter , out_y , out_x] * self.input[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim])\r\n",
        "            d_filter[curr_filter] +=  error_input[curr_filter , out_y , out_x] * self.input[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim]\r\n",
        "\r\n",
        "            #gradiant of the input \r\n",
        "            dout[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim] += error_input[curr_filter , out_y , out_x] * self.filter[curr_filter]\r\n",
        "\r\n",
        "            \r\n",
        "            curr_x += self.strides\r\n",
        "            out_x +=1\r\n",
        "          curr_y += self.strides\r\n",
        "          out_y += 1\r\n",
        "        d_bias[curr_filter] = np.sum(error_input[curr_filter])\r\n",
        "    #update the weights\r\n",
        "        self.filter = d_filter\r\n",
        "        self.bias = d_bias\r\n",
        "      print(dout)\r\n",
        "      return dout\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVoaFlOtZHF6"
      },
      "source": [
        "class MaxPooling2D(con2D):\r\n",
        "  def __init__(self , pool_size = 1, strides=1):\r\n",
        "    self.pool_size = pool_size #getting the kernal size for pooling the layer out\r\n",
        "    self.strides = strides\r\n",
        "    self.image = None\r\n",
        "    \r\n",
        "  def forward_propogation(self , image):\r\n",
        "\r\n",
        "    \"\"\" downsample of the image using the kernal size and strides\"\"\"\r\n",
        "    self.image = image\r\n",
        "    img_channel , h_prev , w_prev  = image.shape    # shape of the image\r\n",
        "    # calculate output dimensions after the maxpooling operation.\r\n",
        "    h = int((h_prev - self.pool_size)/self.strides)+1 \r\n",
        "    w = int((w_prev - self.pool_size)/self.strides)+1\r\n",
        "    \r\n",
        "    # create a matrix to hold the values of the maxpooling operation.\r\n",
        "    self.output = np.zeros((img_channel, h, w))\r\n",
        "    # slide the window over every part of the image using stride s. Take the maximum value at each step.\r\n",
        "    for i in range(img_channel):\r\n",
        "        curr_y = out_y = 0\r\n",
        "        # slide the max pooling window vertically across the image\r\n",
        "        while curr_y + self.pool_size <= h_prev:\r\n",
        "            curr_x = out_x = 0\r\n",
        "            # slide the max pooling window horizontally across the image\r\n",
        "            while curr_x + self.pool_size <= w_prev:\r\n",
        "                # choose the maximum value within the window at each step and store it to the output matrix\r\n",
        "                self.output[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+self.pool_size, curr_x:curr_x+self.pool_size])\r\n",
        "                curr_x += self.strides\r\n",
        "                out_x += 1\r\n",
        "            curr_y += self.strides\r\n",
        "            out_y += 1\r\n",
        "    print(self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def get_max_index(self , arr):\r\n",
        "      \"\"\"return the index of the maximum value in the array\"\"\"\r\n",
        "      flat_index = np.nanargmax(arr)\r\n",
        "      idex = np.unravel_index(flat_index , arr.shape)\r\n",
        "      return idex\r\n",
        "\r\n",
        "\r\n",
        "  def back_propogation(self, output , learning_rate):\r\n",
        "      \"\"\"put the max value in it's old position ,,,fill remaining with zeros\"\"\"\r\n",
        "      img_channel , img_dim , _  = self.image.shape\r\n",
        "\r\n",
        "      #create an zero value array with the original image size\r\n",
        "      dout = np.zeros(self.image.shape)\r\n",
        "      # slide the window over every part of the image using stride s. Take the maximum value at each step.\r\n",
        "      for cuur_channel in range(img_channel):\r\n",
        "        curr_y = out_y = 0\r\n",
        "        # slide the max pooling window vertically across the image\r\n",
        "        while curr_y + self.pool_size <= img_dim:\r\n",
        "            curr_x = out_x = 0\r\n",
        "            # slide the max pooling window horizontally across the image\r\n",
        "            while curr_x + self.pool_size <= img_dim:\r\n",
        "              #obtain the index of the maximum value in that window size\r\n",
        "              a,b = self.get_max_index(self.image[cuur_channel , curr_y:curr_y+self.pool_size, curr_x:curr_x+self.pool_size])\r\n",
        "              dout[cuur_channel , curr_y+a , curr_x+b] = output[cuur_channel , curr_y , curr_x]\r\n",
        "\r\n",
        "              curr_x += self.strides\r\n",
        "              out_x += 1\r\n",
        "            curr_y += self.strides\r\n",
        "            curr_y += 1\r\n",
        "      print(dout)\r\n",
        "      return dout\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKZCutOkcabM"
      },
      "source": [
        "class Flatten(con2D):\r\n",
        "  \"\"\" get the strignt single channel fully connet layer \"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    self.image = None\r\n",
        "    self.flatten_image = None\r\n",
        "    \r\n",
        "  def forward_propogation(self , image):\r\n",
        "    #store the image  size to use in the backpropogation\r\n",
        "    self.image = image\r\n",
        "    image_channel , input_dim , _ = image.shape\r\n",
        "    a = image.flatten()  #flatten the image \r\n",
        "    self.flatten_image = np.array([a]) #make it to 2D\r\n",
        "    print(self.flatten_image)\r\n",
        "    return self.flatten_image \r\n",
        "\r\n",
        "  def back_propogation(self , output , learning_rate):\r\n",
        "    output = output.reshape(self.image.shape)\r\n",
        "    print(output)\r\n",
        "    return output  # mae it into the original size\r\n",
        "    \r\n",
        "      "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ4RKj2mdaGF"
      },
      "source": [
        "class FClayer(Flatten):\r\n",
        "  def __init__(self , input_size , output_size):\r\n",
        "    self.input = None\r\n",
        "    self.input_size = input_size\r\n",
        "    self.output = None\r\n",
        "    # initialize the weights using the normal distribution\r\n",
        "    self.weights = np.random.rand(self.input_size , output_size) - 0.5\r\n",
        "    self.bias =  np.random.rand(1,output_size) - 0.5 \r\n",
        "  \r\n",
        "  def forward_propogation(self , input):\r\n",
        "    self.input = input\r\n",
        "    self.output = np.dot(self.input , self.weights) + self.bias\r\n",
        "    print(self.output)\r\n",
        "    return self.output\r\n",
        "  \r\n",
        "  def back_propogation(self, output_error , learning_rate):\r\n",
        "    print('coming gradiant' , output_error)\r\n",
        "    print(' weight' , self.weights)\r\n",
        "    print('back input ',self.input)\r\n",
        "    print('bias',self.bias)\r\n",
        "    print('finding the input error')\r\n",
        "    print('the weight transpose' , self.weights.T)\r\n",
        "    print('error',output_error ,'*' ,self.weights.T )\r\n",
        "    input_error = np.dot(output_error , self.weights.T)  #dE/dX = W.T * dE / dY\r\n",
        "    \r\n",
        "    \r\n",
        "    print('input transpose',self.input.T)\r\n",
        "    weights_error = np.dot(self.input.T , output_error) #dE/dW = X.T * dE / dY\r\n",
        "    bias_error = output_error                           #dE/dB =  dE / dY\r\n",
        "    #update the weights and bias\r\n",
        "    self.weights -= learning_rate *  weights_error\r\n",
        "    self.bias -= learning_rate * bias_error\r\n",
        "    print(input_error)\r\n",
        "    return input_error\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bQtsIVFXBzv"
      },
      "source": [
        "def softmax_funtion(output):\r\n",
        "  \"\"\" pass the raw predicted data to the activation function to get the probability\"\"\"\r\n",
        "  out = np.exp(output) # exponent the predicted output\r\n",
        "  return out / np.sum(out)\r\n",
        "\r\n",
        "def softmax_derivated(loss_derivated_error):\r\n",
        "  return loss_derivated_error\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHWBhiT-XT3z"
      },
      "source": [
        "def catogarical_cross_entropy(prob , label):\r\n",
        "  \"\"\" calculate the categorical cross entropy loss\"\"\"\r\n",
        "  return -np.sum(label * np.log(prob)) #multiply the output with the log of probability\r\n",
        "\r\n",
        "def catogarical_cross_entropy_derivated(prob , label):\r\n",
        "  loss_derivated_error = prob / label\r\n",
        "  print('loss derivated',loss_derivated_error)\r\n",
        "  return loss_derivated_error"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnUTA81OVPh1"
      },
      "source": [
        "class softmax(FClayer):\r\n",
        "  def __init__(self, actvation = softmax_funtion , activation_derivated = softmax_derivated): #default parameter setting\r\n",
        "    self.actvation = actvation #show the activation from the outside class\r\n",
        "    self.activation_derivated = activation_derivated\r\n",
        "\r\n",
        "  def forward_propogation(self , input_data):\r\n",
        "    self.input = input_data \r\n",
        "    self.output = self.actvation(self.input)\r\n",
        "    print('soft max activation',self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self ,  output_error , learning_rate):\r\n",
        "    return self.activation_derivated(self.input) * output_error"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxpXaBr2ZDgW"
      },
      "source": [
        "class Relu(con2D):\r\n",
        "  \"\"\" pass through the relu activation function\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    self.output = None\r\n",
        "\r\n",
        "  def forward_propogation(self,input):\r\n",
        "    input[input<=0] = 0\r\n",
        "    self.output = input\r\n",
        "    print(self.output)\r\n",
        "    return self.output\r\n",
        "  \r\n",
        "  def back_propogation(self,output_error , learning_rate):\r\n",
        "    output_error[output_error<=0] = 0\r\n",
        "    return output_error\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7KVBmaF8oC"
      },
      "source": [
        "class network:\r\n",
        "  def __init__(self):\r\n",
        "    self.layers = []\r\n",
        "\r\n",
        "  #add the layer in the model\r\n",
        "  def add(self , layer):\r\n",
        "    self.layers.append(layer)\r\n",
        "  #set the loss function to use it out\r\n",
        "  def setloss(self, loss , loss_derivated):\r\n",
        "    self.loss = loss\r\n",
        "    self.loss_derivated = loss_derivated\r\n",
        "\r\n",
        "  def fit(self , x_train ,  y_train , epoches ,learning_rate):\r\n",
        "    sampels = len(x_train)\r\n",
        "    #loop over for epoches\r\n",
        "    for i in range(epoches):\r\n",
        "      err = 0\r\n",
        "      for j in range(sampels):\r\n",
        "        print('sample' , j)\r\n",
        "        output = x_train[j] \r\n",
        "        for layer in self.layers:\r\n",
        "          print(layer)\r\n",
        "          output = layer.forward_propogation(output)\r\n",
        "\r\n",
        "        #compute the loss\r\n",
        "        err += self.loss(y_train[j] , output)\r\n",
        "        print('loss ',err)\r\n",
        "        #backpropogation\r\n",
        "        error = self.loss_derivated(y_train[j] , output)\r\n",
        "        \r\n",
        "        #update the weights\r\n",
        "        print('backporopogation stated')\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "          print('back',layer)\r\n",
        "          error = layer.back_propogation(error , learning_rate)\r\n",
        "      #displae the error and epoches for the reference\r\n",
        "      err/=sampels\r\n",
        "      print(err)\r\n",
        "    print(output)\r\n",
        "\r\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtEJ1g6OHtBm"
      },
      "source": [
        "#build the model \r\n",
        "model = network()\r\n",
        "model.add(con2D(filter = 1 , kernal_size =(3,3) , strides = (1,1) , img_channel =1))\r\n",
        "model.add(Relu())\r\n",
        "model.add(MaxPooling2D(pool_size = 2))\r\n",
        "model.add(Relu())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(FClayer(4,3))\r\n",
        "model.add(FClayer(3,2))\r\n",
        "model.add(softmax())"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAc1cZ1GMI70",
        "outputId": "04a71a14-7adc-4e10-b8ce-606e2ce4b962"
      },
      "source": [
        "i = np.array([[[[1,2,1,0,1] , [2,1,1,2,1] , [3,2,1,0,0] , [0,1,2,3,1] , [2,1,1,0,2]]]])\r\n",
        "print(i.shape)\r\n",
        "#set the loss to compute the loss\r\n",
        "model.setloss(loss = catogarical_cross_entropy , loss_derivated = catogarical_cross_entropy_derivated)\r\n",
        "model.fit(i ,[1,0] ,1 , learning_rate = 0.01)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 5, 5)\n",
            "sample 0\n",
            "<__main__.con2D object at 0x7f25c710d410>\n",
            "out put dimention 3\n",
            "[[[1.45532157 1.9195338  0.69920107]\n",
            "  [2.15873956 1.45775738 0.98596288]\n",
            "  [2.14337815 1.96910697 1.5599534 ]]]\n",
            "<__main__.Relu object at 0x7f25c6ad19d0>\n",
            "[[[1.45532157 1.9195338  0.69920107]\n",
            "  [2.15873956 1.45775738 0.98596288]\n",
            "  [2.14337815 1.96910697 1.5599534 ]]]\n",
            "<__main__.MaxPooling2D object at 0x7f25c6ad1c50>\n",
            "[[[2.15873956 1.9195338 ]\n",
            "  [2.15873956 1.96910697]]]\n",
            "<__main__.Relu object at 0x7f25cd869890>\n",
            "[[[2.15873956 1.9195338 ]\n",
            "  [2.15873956 1.96910697]]]\n",
            "<__main__.Flatten object at 0x7f25c6b0ad10>\n",
            "[[2.15873956 1.9195338  2.15873956 1.96910697]]\n",
            "<__main__.FClayer object at 0x7f25c6a9a650>\n",
            "[[-0.52070406 -0.1216365   2.41217346]]\n",
            "<__main__.FClayer object at 0x7f25c6a9af50>\n",
            "[[ 0.13154152 -0.33334135]]\n",
            "<__main__.softmax object at 0x7f25c6a9a750>\n",
            "soft max activation [[0.61417189 0.38582811]]\n",
            "loss  0.0\n",
            "loss derivated [[1.62820868 2.59182774]]\n",
            "backporopogation stated\n",
            "back <__main__.softmax object at 0x7f25c6a9a750>\n",
            "back <__main__.FClayer object at 0x7f25c6a9af50>\n",
            "coming gradiant [[ 0.21417705 -0.86396337]]\n",
            " weight [[-0.46927596 -0.33909704]\n",
            " [ 0.08598564  0.10928827]\n",
            " [ 0.09664921 -0.11697013]]\n",
            "back input  [[-0.52070406 -0.1216365   2.41217346]]\n",
            "bias [[-0.33548804 -0.21446486]]\n",
            "finding the input error\n",
            "the weight transpose [[-0.46927596  0.08598564  0.09664921]\n",
            " [-0.33909704  0.10928827 -0.11697013]]\n",
            "error [[ 0.21417705 -0.86396337]] * [[-0.46927596  0.08598564  0.09664921]\n",
            " [-0.33909704  0.10928827 -0.11697013]]\n",
            "input transpose [[-0.52070406]\n",
            " [-0.1216365 ]\n",
            " [ 2.41217346]]\n",
            "[[ 0.19245928 -0.07600491  0.12175795]]\n",
            "back <__main__.FClayer object at 0x7f25c6a9a650>\n",
            "coming gradiant [[ 0.19245928 -0.07600491  0.12175795]]\n",
            " weight [[-0.07664902  0.47727647  0.04609478]\n",
            " [-0.43932843  0.10375685  0.37908853]\n",
            " [ 0.4234131  -0.44023133  0.14446725]\n",
            " [-0.45655599 -0.35836166  0.41134152]]\n",
            "back input  [[2.15873956 1.9195338  2.15873956 1.96910697]]\n",
            "bias [[0.47303595 0.30488035 0.46315098]]\n",
            "finding the input error\n",
            "the weight transpose [[-0.07664902 -0.43932843  0.4234131  -0.45655599]\n",
            " [ 0.47727647  0.10375685 -0.44023133 -0.35836166]\n",
            " [ 0.04609478  0.37908853  0.14446725  0.41134152]]\n",
            "error [[ 0.19245928 -0.07600491  0.12175795]] * [[-0.07664902 -0.43932843  0.4234131  -0.45655599]\n",
            " [ 0.47727647  0.10375685 -0.44023133 -0.35836166]\n",
            " [ 0.04609478  0.37908853  0.14446725  0.41134152]]\n",
            "input transpose [[2.15873956]\n",
            " [1.9195338 ]\n",
            " [2.15873956]\n",
            " [1.96910697]]\n",
            "[[-0.04541476 -0.04628182  0.13253956 -0.01054709]]\n",
            "back <__main__.Flatten object at 0x7f25c6b0ad10>\n",
            "[[[-0.04541476 -0.04628182]\n",
            "  [ 0.13253956 -0.01054709]]]\n",
            "back <__main__.Relu object at 0x7f25cd869890>\n",
            "back <__main__.MaxPooling2D object at 0x7f25c6ad1c50>\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "back <__main__.Relu object at 0x7f25c6ad19d0>\n",
            "back <__main__.con2D object at 0x7f25c710d410>\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "filter [[[[ 0.5320522  -0.13674772 -0.1864958 ]\n",
            "   [ 0.2706675  -0.16781733  0.31800117]\n",
            "   [ 0.10061066  0.05210331  0.28570318]]]]\n",
            "zero filter [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]]]\n",
            "0.0\n",
            "[[0.61417189 0.38582811]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM_PPS7JkOmW"
      },
      "source": [
        "f_size = (2,3,3)\r\n",
        "sdv = 1.0 / np.sqrt(np.prod(f_size))\r\n",
        "sd = np.random.normal(loc = 0, scale = sdv, size = f_size)\r\n",
        "sd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpvgvIc-kQrb"
      },
      "source": [
        "s = np.array([[[1,0,1] , [2,1,1] , [0,1,2]]])\r\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHIigI3DugQ7"
      },
      "source": [
        "i = np.array([ [[1,2,1,0,-1] , [-2,1,1,2,1] , [3,2,1,0,0] , [0,1,2,3,1] , [2,1,1,0,2]]])\r\n",
        "i[i<=0] = 0\r\n",
        "i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "vg5WFQJ3LmR_",
        "outputId": "33eec8c9-8917-4c90-e259-de47806f7149"
      },
      "source": [
        "class a:\r\n",
        "   \r\n",
        "  def __init__(self):\r\n",
        "    self.i = 2\r\n",
        "  def d(self):\r\n",
        "    self.i = 10\r\n",
        "    print(self.i)\r\n",
        "  def geti(self):\r\n",
        "    return self.i\r\n",
        "a().d()  \r\n",
        "class b(a):\r\n",
        "  print(super().geti())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-a040f893a167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-a040f893a167>\u001b[0m in \u001b[0;36mb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: super(): no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi8081s7Ln_6",
        "outputId": "e6285203-7262-416a-fe63-e539f696322a"
      },
      "source": [
        "s= np.array([[-0.06959409 , 0.12889191],\r\n",
        " [-0.17250638 ,-0.45558094],\r\n",
        " [ 0.05320838 ,-0.37098399]])\r\n",
        "s.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06959409, -0.17250638,  0.05320838],\n",
              "       [ 0.12889191, -0.45558094, -0.37098399]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82kVP018SC0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0a43a3-87c3-4980-d403-7f151e7ef6cf"
      },
      "source": [
        "a= [17., 16., 17., 16.]\r\n",
        "s= np.array([a])\r\n",
        "s.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17.],\n",
              "       [16.],\n",
              "       [17.],\n",
              "       [16.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPCfdrQShnAK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}