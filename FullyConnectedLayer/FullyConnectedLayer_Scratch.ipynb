{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FullyConnectedLayer_Scratch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNh5h2vgONExB/V//kQhph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/NuralNetwork_scratch/blob/main/FullyConnectedLayer/FullyConnectedLayer_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvU7DvCCReTN"
      },
      "source": [
        "#gonna train the XOR gate from scratch\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E--1Eqb-RsUT"
      },
      "source": [
        "class FClayer:\r\n",
        "  def __init__(self , input_size , output_size):\r\n",
        "    self.input = None\r\n",
        "    self.output = None\r\n",
        "    #rondomly initialize the weights\r\n",
        "    self.weights = np.random.rand(input_size , output_size) - 0.5\r\n",
        "    self.bias =  np.random.rand(1,output_size) - 0.5 \r\n",
        "\r\n",
        "  def forward_propogation(self , input):\r\n",
        "    self.input = input\r\n",
        "    self.output = np.dot(self.input , self.weights) + self.bias\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self , output_error , learning_rate):\r\n",
        "    input_error = np.dot(output_error , self.weights.T)\r\n",
        "    weights_error = np.dot(self.input.T , output_error)\r\n",
        "    bias_error = output_error\r\n",
        "    #update the weights and bias\r\n",
        "    self.weights -= learning_rate *  weights_error\r\n",
        "    self.bias -= learning_rate * bias_error\r\n",
        "\r\n",
        "    return input_error"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo6_1e9dULiX"
      },
      "source": [
        "#activation layer\r\n",
        "#inherite the fully connected layer\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class ActivationLayer(FClayer):\r\n",
        "  def __init__(self, actvation , activation_derivated):\r\n",
        "    self.actvation = actvation\r\n",
        "    self.activation_derivated = activation_derivated\r\n",
        "\r\n",
        "  def forward_propogation(self , input_data):\r\n",
        "    self.input = input_data\r\n",
        "    self.output = self.actvation(self.input)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self ,  output_error , learning_rate):\r\n",
        "    return self.activation_derivated(self.input) . output_error"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wphc1mFoZZXe"
      },
      "source": [
        "def tanh(x):\r\n",
        "  return np.tanh(x)\r\n",
        "\r\n",
        "def tanh_derivated(x):\r\n",
        "  return 1-np.tanh(x) ** 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai95HPyUZsWP"
      },
      "source": [
        "#defining the lose\r\n",
        "\r\n",
        "def mse(y_true , y_pred):\r\n",
        "  return np.mean(np.power(y_true - y_pred , 2))\r\n",
        "\r\n",
        "def mse_derivated(y_true , y_pred):\r\n",
        "  return 2*(y_pred - y_true) / y_true.size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_vLQIzdaOyL"
      },
      "source": [
        "#defning the network or model\r\n",
        "\r\n",
        "class model:\r\n",
        "  def __init__(self):\r\n",
        "    self.layers = []\r\n",
        "    self.loss = None\r\n",
        "    self.loss_derivated = None\r\n",
        "\r\n",
        "  #adding layer to the model\r\n",
        "  def add(self , layer):\r\n",
        "    self.layers.append(layer)\r\n",
        "\r\n",
        "  def setloss(self, loss , loss_derivated):\r\n",
        "    self.loss = loss\r\n",
        "    self.loss_derivated = loss_derivated\r\n",
        "\r\n",
        "  #predict the unknow data\r\n",
        "  def predict(self , Newinput):\r\n",
        "    samples = len(Newinput)\r\n",
        "    result = []\r\n",
        "    for i in range(samples):\r\n",
        "      output = Newinput[i]\r\n",
        "      \r\n",
        "      for layer in self.layers:\r\n",
        "        ouput = layer.forward_propogation(output)\r\n",
        "\r\n",
        "      result.append(output)\r\n",
        "    return result\r\n",
        "  \r\n",
        "  def fit(self , x_train , y_train , epoches , learning_rate):\r\n",
        "    sampels = len(x_train)\r\n",
        "    for i in epoches:\r\n",
        "      err = 0\r\n",
        "      for j in range(sampels):\r\n",
        "        output = x_train[j] \r\n",
        "        for layer in self.layers:\r\n",
        "          ouput = layer.forward_propogation(output)\r\n",
        "        \r\n",
        "        #compute the loss\r\n",
        "        err += self.loss(y_train[j] , output)\r\n",
        "\r\n",
        "        #backpropogation\r\n",
        "        error = self.loss_derivated(y_train[j] , output)\r\n",
        "\r\n",
        "        #update the weights\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "          error = layer.back_propogation(error , learning_rate)\r\n",
        "        \r\n",
        "    #calculate the average error and display that\r\n",
        "      err /= sampels\r\n",
        "      print('epoches..'+j+'/'+epoches+'-------'+'Error'+err)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XbA3qhdph9D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}