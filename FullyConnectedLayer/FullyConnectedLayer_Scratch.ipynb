{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FullyConnectedLayer_Scratch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4FqT3Ah+NammZRdRBbEbS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/NuralNetwork_scratch/blob/main/FullyConnectedLayer/FullyConnectedLayer_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvU7DvCCReTN"
      },
      "source": [
        "#gonna train the XOR gate from scratch\r\n",
        "import numpy as np"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E--1Eqb-RsUT"
      },
      "source": [
        "class FClayer:\r\n",
        "  def __init__(self , input_size , output_size):\r\n",
        "    self.input = None\r\n",
        "    self.output = None\r\n",
        "    #rondomly initialize the weights\r\n",
        "    self.weights = np.random.rand(input_size , output_size) - 0.5\r\n",
        "    self.bias =  np.random.rand(1,output_size) - 0.5 \r\n",
        "\r\n",
        "  def forward_propogation(self , input):\r\n",
        "    self.input = input\r\n",
        "    #print('weights',self.weights)\r\n",
        "    #print('bias',self.bias)\r\n",
        "    self.output = np.dot(self.input , self.weights) + self.bias\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self , output_error , learning_rate):\r\n",
        "    input_error = np.dot(output_error , self.weights.T)\r\n",
        "    #print('back weight' , self.weights)\r\n",
        "    weights_error = np.dot(self.input.T , output_error)\r\n",
        "    bias_error = output_error\r\n",
        "    #update the weights and bias\r\n",
        "    self.weights -= learning_rate *  weights_error\r\n",
        "    self.bias -= learning_rate * bias_error\r\n",
        "\r\n",
        "    return input_error"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo6_1e9dULiX"
      },
      "source": [
        "#activation layer\r\n",
        "#inherite the fully connected layer\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class ActivationLayer(FClayer):\r\n",
        "  def __init__(self, actvation , activation_derivated):\r\n",
        "    self.actvation = actvation\r\n",
        "    self.activation_derivated = activation_derivated\r\n",
        "\r\n",
        "  def forward_propogation(self , input_data):\r\n",
        "    self.input = input_data\r\n",
        "    self.output = self.actvation(self.input)\r\n",
        "    #print('activation' , self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self ,  output_error , learning_rate):\r\n",
        "    return self.activation_derivated(self.input) * output_error"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wphc1mFoZZXe"
      },
      "source": [
        "def tanh(x):\r\n",
        "  return np.tanh(x)\r\n",
        "\r\n",
        "def tanh_derivated(x):\r\n",
        "  return 1-np.tanh(x) ** 2"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai95HPyUZsWP"
      },
      "source": [
        "#defining the lose\r\n",
        "\r\n",
        "def mse(y_true , y_pred):\r\n",
        "  return np.mean(np.power(y_true - y_pred , 2))\r\n",
        "\r\n",
        "def mse_derivated(y_true , y_pred):\r\n",
        "  return 2*(y_pred - y_true) / y_true.size"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_vLQIzdaOyL"
      },
      "source": [
        "#defning the network or model\r\n",
        "\r\n",
        "class model:\r\n",
        "  def __init__(self):\r\n",
        "    self.layers = []\r\n",
        "    self.loss = None\r\n",
        "    self.loss_derivated = None\r\n",
        "\r\n",
        "  #adding layer to the model\r\n",
        "  def add(self , layer):\r\n",
        "    self.layers.append(layer)\r\n",
        "\r\n",
        "  def setloss(self, loss , loss_derivated):\r\n",
        "    self.loss = loss\r\n",
        "    self.loss_derivated = loss_derivated\r\n",
        "\r\n",
        "  #predict the unknow data\r\n",
        "  def predict(self , Newinput):\r\n",
        "    samples = len(Newinput)\r\n",
        "    result = []\r\n",
        "    for i in range(samples):\r\n",
        "      output = Newinput[i]\r\n",
        "      \r\n",
        "      for layer in self.layers:\r\n",
        "        output = layer.forward_propogation(output)\r\n",
        "\r\n",
        "      result.append(output)\r\n",
        "    return result\r\n",
        "  \r\n",
        "  def fit(self , x_train , y_train , epoches , learning_rate):\r\n",
        "    sampels = len(x_train)\r\n",
        "    for i in range(epoches):\r\n",
        "      err = 0\r\n",
        "      for j in range(sampels):\r\n",
        "        output = x_train[j]\r\n",
        "        #print(output , j) \r\n",
        "        for layer in self.layers:\r\n",
        "          #print(layer)\r\n",
        "          output = layer.forward_propogation(output)\r\n",
        "          #print(output)\r\n",
        "        \r\n",
        "        #compute the loss\r\n",
        "        err += self.loss(y_train[j] , output)\r\n",
        "\r\n",
        "        #backpropogation\r\n",
        "        error = self.loss_derivated(y_train[j] , output)\r\n",
        "\r\n",
        "        #update the weights\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "          error = layer.back_propogation(error , learning_rate)\r\n",
        "        \r\n",
        "    #calculate the average error and display that\r\n",
        "      err /= sampels\r\n",
        "      print('epoches..'+str(i)+'/'+str(epoches)+'-------'+'Error',err)\r\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XbA3qhdph9D"
      },
      "source": [
        "#lets try with XOR operation to check it works fine\r\n",
        "X_train = np.array([[[0,0]] , [[0,1]] , [[1,0]] , [[1,1]]])\r\n",
        "y_train = np.array([[[0]] , [[1]] , [[1]] , [[0]]])\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaISs2VRK5rq",
        "outputId": "05ebbd91-1b9d-44c9-9f34-14ba33ccce71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#build the network\r\n",
        "model = model()\r\n",
        "model.add(FClayer(2,3))\r\n",
        "model.add(ActivationLayer(tanh,tanh_derivated))\r\n",
        "model.add(FClayer(3,1))\r\n",
        "model.add(ActivationLayer(tanh,tanh_derivated))\r\n",
        "\r\n",
        "model.setloss(mse , mse_derivated)\r\n",
        "model.fit(X_train , y_train , epoches = 100 , learning_rate = 0.1)\r\n",
        "\r\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoches..0/100-------Error 0.360164386078472\n",
            "epoches..1/100-------Error 0.29212819495295106\n",
            "epoches..2/100-------Error 0.2840265248360428\n",
            "epoches..3/100-------Error 0.2817389961238408\n",
            "epoches..4/100-------Error 0.28046958188007665\n",
            "epoches..5/100-------Error 0.2794365250697789\n",
            "epoches..6/100-------Error 0.2784601032321391\n",
            "epoches..7/100-------Error 0.2774879181196721\n",
            "epoches..8/100-------Error 0.2765019461910285\n",
            "epoches..9/100-------Error 0.2754954447589436\n",
            "epoches..10/100-------Error 0.2744659730665751\n",
            "epoches..11/100-------Error 0.27341294339819205\n",
            "epoches..12/100-------Error 0.2723366743309427\n",
            "epoches..13/100-------Error 0.2712380029659295\n",
            "epoches..14/100-------Error 0.270118119038941\n",
            "epoches..15/100-------Error 0.26897848926224455\n",
            "epoches..16/100-------Error 0.2678208178839935\n",
            "epoches..17/100-------Error 0.2666470207354268\n",
            "epoches..18/100-------Error 0.2654592030966035\n",
            "epoches..19/100-------Error 0.2642596372928131\n",
            "epoches..20/100-------Error 0.26305073838308746\n",
            "epoches..21/100-------Error 0.2618350374280437\n",
            "epoches..22/100-------Error 0.26061515238635274\n",
            "epoches..23/100-------Error 0.25939375700529665\n",
            "epoches..24/100-------Error 0.25817354827136807\n",
            "epoches..25/100-------Error 0.2569572131221909\n",
            "epoches..26/100-------Error 0.2557473952085558\n",
            "epoches..27/100-------Error 0.25454666253934843\n",
            "epoches..28/100-------Error 0.25335747684291426\n",
            "epoches..29/100-------Error 0.25218216543631916\n",
            "epoches..30/100-------Error 0.25102289631130403\n",
            "epoches..31/100-------Error 0.2498816570273295\n",
            "epoches..32/100-------Error 0.24876023785532814\n",
            "epoches..33/100-------Error 0.2476602194499974\n",
            "epoches..34/100-------Error 0.24658296515429257\n",
            "epoches..35/100-------Error 0.24552961786807975\n",
            "epoches..36/100-------Error 0.24450110125386926\n",
            "epoches..37/100-------Error 0.24349812491478393\n",
            "epoches..38/100-------Error 0.2425211930698544\n",
            "epoches..39/100-------Error 0.2415706161732565\n",
            "epoches..40/100-------Error 0.24064652487845684\n",
            "epoches..41/100-------Error 0.23974888573426248\n",
            "epoches..42/100-------Error 0.23887751801430201\n",
            "epoches..43/100-------Error 0.2380321111198705\n",
            "epoches..44/100-------Error 0.23721224205285407\n",
            "epoches..45/100-------Error 0.2364173925248378\n",
            "epoches..46/100-------Error 0.23564696534486998\n",
            "epoches..47/100-------Error 0.2349002998067386\n",
            "epoches..48/100-------Error 0.23417668587277546\n",
            "epoches..49/100-------Error 0.2334753770219326\n",
            "epoches..50/100-------Error 0.23279560169287175\n",
            "epoches..51/100-------Error 0.2321365733067199\n",
            "epoches..52/100-------Error 0.23149749889835664\n",
            "epoches..53/100-------Error 0.2308775864196942\n",
            "epoches..54/100-------Error 0.23027605080388278\n",
            "epoches..55/100-------Error 0.2296921188966129\n",
            "epoches..56/100-------Error 0.2291250333707444\n",
            "epoches..57/100-------Error 0.22857405574452017\n",
            "epoches..58/100-------Error 0.22803846862279076\n",
            "epoches..59/100-------Error 0.22751757727609034\n",
            "epoches..60/100-------Error 0.2270107106650654\n",
            "epoches..61/100-------Error 0.22651722200857022\n",
            "epoches..62/100-------Error 0.22603648898345263\n",
            "epoches..63/100-------Error 0.2255679136332902\n",
            "epoches..64/100-------Error 0.22511092205260108\n",
            "epoches..65/100-------Error 0.22466496390271717\n",
            "epoches..66/100-------Error 0.22422951180585568\n",
            "epoches..67/100-------Error 0.22380406065513325\n",
            "epoches..68/100-------Error 0.2233881268704562\n",
            "epoches..69/100-------Error 0.2229812476234169\n",
            "epoches..70/100-------Error 0.2225829800485542\n",
            "epoches..71/100-------Error 0.22219290045352685\n",
            "epoches..72/100-------Error 0.22181060353684862\n",
            "epoches..73/100-------Error 0.2214357016187566\n",
            "epoches..74/100-------Error 0.22106782388842539\n",
            "epoches..75/100-------Error 0.22070661566900135\n",
            "epoches..76/100-------Error 0.22035173770071254\n",
            "epoches..77/100-------Error 0.22000286544151845\n",
            "epoches..78/100-------Error 0.2196596883843008\n",
            "epoches..79/100-------Error 0.2193219093893941\n",
            "epoches..80/100-------Error 0.21898924403124106\n",
            "epoches..81/100-------Error 0.21866141995806243\n",
            "epoches..82/100-------Error 0.21833817626362906\n",
            "epoches..83/100-------Error 0.2180192628704513\n",
            "epoches..84/100-------Error 0.21770443992394561\n",
            "epoches..85/100-------Error 0.21739347719737356\n",
            "epoches..86/100-------Error 0.21708615350755375\n",
            "epoches..87/100-------Error 0.21678225614152224\n",
            "epoches..88/100-------Error 0.21648158029445086\n",
            "epoches..89/100-------Error 0.2161839285192221\n",
            "epoches..90/100-------Error 0.21588911018811613\n",
            "epoches..91/100-------Error 0.21559694096707738\n",
            "epoches..92/100-------Error 0.21530724230301437\n",
            "epoches..93/100-------Error 0.2150198409245418\n",
            "epoches..94/100-------Error 0.21473456835650875\n",
            "epoches..95/100-------Error 0.21445126044857765\n",
            "epoches..96/100-------Error 0.2141697569180246\n",
            "epoches..97/100-------Error 0.21388990090683518\n",
            "epoches..98/100-------Error 0.21361153855307005\n",
            "epoches..99/100-------Error 0.21333451857637642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2_ixp8L3uu",
        "outputId": "6421d72d-ecdd-428a-faa4-6f2a9600a20b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = model.predict(X_train)\r\n",
        "y_pred"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.07107528]]),\n",
              " array([[0.66072806]]),\n",
              " array([[0.65371965]]),\n",
              " array([[0.71388975]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C2FjuwagTDF"
      },
      "source": [
        "#almost correct more epoches will bring better than this"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxHHGm-thAGw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}