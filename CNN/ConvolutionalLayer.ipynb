{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionalLayer",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/pavi-ninjaac/NuralNetwork_scratch/blob/main/CNN/ConvolutionalLayer.ipynb",
      "authorship_tag": "ABX9TyM53/wzU66b2/JFiE0rTHKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/NuralNetwork_scratch/blob/main/CNN/ConvolutionalLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IpwmtBke1F9"
      },
      "source": [
        "# conolutional layer\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUignDQe8Vv"
      },
      "source": [
        "class con2D:\r\n",
        "  def __init__(self, filter , kernal_size, img_channel,strides = (1,1)):\r\n",
        "    self.filter = None\r\n",
        "    self.bias = None\r\n",
        "    self.input = None\r\n",
        "    \"\"\"Convolute the image with the filter and strids\"\"\"\r\n",
        "    \r\n",
        "    \r\n",
        "    f_channel = img_channel               #filter channel must match with the image channel\r\n",
        "    self.no_filters = filter                   # number of filters\r\n",
        "    f_dim , _ = kernal_size\r\n",
        "    self.f_dim =  f_dim              # get the filter size\r\n",
        "    self.strides , _ = strides\r\n",
        "    f_size = (self.no_filters , f_channel , f_dim , f_dim) # get the strides dimentions\r\n",
        "\r\n",
        "    #initialize the filter\r\n",
        "    \"\"\" Initalize the filter with the noraml distribution with the standard deviation inversely proportional the square root of the number of units\"\"\"\r\n",
        "    sdv = 1.0 / np.sqrt(np.prod(f_size))\r\n",
        "    self.filter = np.random.normal(loc = 0, scale = sdv, size = f_size)\r\n",
        "    #self.filter = np.array([[[1,0,1] , [2,1,1] , [0,1,2]]])\r\n",
        "    self.bias = np.zeros((self.no_filters , 1)) # each for the one filters    \r\n",
        "\r\n",
        "  \r\n",
        "  #forward propogaton\r\n",
        "  def forward_propogation(self, image):\r\n",
        "    self.input = image\r\n",
        "    #print(image.shape)\r\n",
        "    (img_channel , input_dim , _ ) = image.shape #input dimensions channel first dimention \r\n",
        "    \r\n",
        "    #calculate the output dimension, so according to that i can create the empty array to store value\r\n",
        "    out_dim = int(((input_dim - self.f_dim) / self.strides) + 1)\r\n",
        "    \r\n",
        "    #create the empty array with output dimetions\r\n",
        "    self.output = np.zeros((1 , out_dim, out_dim )) #syntax np.zeros(channel , row , col)\r\n",
        "    for curr_filter in range(self.no_filters): # for loop over the number of filters\r\n",
        "      curr_y = out_y =0\r\n",
        "      \r\n",
        "      #move vertically over the image\r\n",
        "      while (curr_y + self.f_dim) <= input_dim:\r\n",
        "        curr_x = out_x = 0\r\n",
        "        #move horizondally\r\n",
        "        while (curr_x + self.f_dim) <= input_dim:\r\n",
        "          \r\n",
        "          \"\"\"perform convolution operation on the image\"\"\"\r\n",
        "          self.output[curr_filter , curr_y , curr_x] = np.sum(self.filter[curr_filter] * image[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim]) #+ self.bias[curr_filter]\r\n",
        "\r\n",
        "          curr_x += self.strides\r\n",
        "          out_x +=1\r\n",
        "        curr_y += self.strides\r\n",
        "        out_y += 1\r\n",
        "    #print(self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self , error_input , learning_rate):\r\n",
        "      \"\"\"update the filter \"\"\" # gonna use the self.no_filters and self.f_dim\r\n",
        "      image_channel , img_dim , _ = self.input.shape\r\n",
        "\r\n",
        "      #initialize the derivations by creating zero array\r\n",
        "      dout = np.zeros(self.input.shape)\r\n",
        "      d_filter = np.zeros(self.filter.shape)\r\n",
        "      d_bias = np.zeros(self.bias.shape)\r\n",
        "      for curr_filter in range(self.no_filters): # for loop over the number of filters\r\n",
        "        curr_y = out_y =0\r\n",
        "      \r\n",
        "        #move vertically over the image\r\n",
        "        while (curr_y + self.f_dim) <= img_dim:\r\n",
        "          curr_x = out_x = 0\r\n",
        "          #move horizondally\r\n",
        "          while (curr_x + self.f_dim) <= img_dim:\r\n",
        "          \r\n",
        "            \"\"\"loss gradiant on filters\"\"\"\r\n",
        "            #update the filter\r\n",
        "            #print('filter' , self.filter)\r\n",
        "            #print('zero filter' , d_filter)\r\n",
        "            d_filter[curr_filter] +=  error_input[curr_filter , out_y , out_x] * self.input[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim]\r\n",
        "\r\n",
        "            #gradiant of the input \r\n",
        "            dout[: , curr_y : curr_y + self.f_dim , curr_x : curr_x + self.f_dim] += error_input[curr_filter , out_y , out_x] * self.filter[curr_filter]\r\n",
        "\r\n",
        "            \r\n",
        "            curr_x += self.strides\r\n",
        "            out_x +=1\r\n",
        "          curr_y += self.strides\r\n",
        "          out_y += 1\r\n",
        "        d_bias[curr_filter] = np.sum(error_input[curr_filter])\r\n",
        "    #update the weights\r\n",
        "        self.filter = d_filter\r\n",
        "        self.bias = d_bias\r\n",
        "      #print(dout)\r\n",
        "      return dout\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVoaFlOtZHF6"
      },
      "source": [
        "class MaxPooling2D(con2D):\r\n",
        "  def __init__(self , pool_size = 1, strides=1):\r\n",
        "    self.pool_size = pool_size #getting the kernal size for pooling the layer out\r\n",
        "    self.strides = strides\r\n",
        "    self.image = None\r\n",
        "    \r\n",
        "  def forward_propogation(self , image):\r\n",
        "\r\n",
        "    \"\"\" downsample of the image using the kernal size and strides\"\"\"\r\n",
        "    self.image = image\r\n",
        "    img_channel , h_prev , w_prev  = image.shape    # shape of the image\r\n",
        "    # calculate output dimensions after the maxpooling operation.\r\n",
        "    h = int((h_prev - self.pool_size)/self.strides)+1 \r\n",
        "    w = int((w_prev - self.pool_size)/self.strides)+1\r\n",
        "    \r\n",
        "    # create a matrix to hold the values of the maxpooling operation.\r\n",
        "    self.output = np.zeros((img_channel, h, w))\r\n",
        "    # slide the window over every part of the image using stride s. Take the maximum value at each step.\r\n",
        "    for i in range(img_channel):\r\n",
        "        curr_y = out_y = 0\r\n",
        "        # slide the max pooling window vertically across the image\r\n",
        "        while curr_y + self.pool_size <= h_prev:\r\n",
        "            curr_x = out_x = 0\r\n",
        "            # slide the max pooling window horizontally across the image\r\n",
        "            while curr_x + self.pool_size <= w_prev:\r\n",
        "                # choose the maximum value within the window at each step and store it to the output matrix\r\n",
        "                self.output[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+self.pool_size, curr_x:curr_x+self.pool_size])\r\n",
        "                curr_x += self.strides\r\n",
        "                out_x += 1\r\n",
        "            curr_y += self.strides\r\n",
        "            out_y += 1\r\n",
        "    #print(self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def get_max_index(self , arr):\r\n",
        "      \"\"\"return the index of the maximum value in the array\"\"\"\r\n",
        "      flat_index = np.nanargmax(arr)\r\n",
        "      idex = np.unravel_index(flat_index , arr.shape)\r\n",
        "      return idex\r\n",
        "\r\n",
        "\r\n",
        "  def back_propogation(self, output , learning_rate):\r\n",
        "      \"\"\"put the max value in it's old position ,,,fill remaining with zeros\"\"\"\r\n",
        "      img_channel , img_dim , _  = self.image.shape\r\n",
        "\r\n",
        "      #create an zero value array with the original image size\r\n",
        "      dout = np.zeros(self.image.shape)\r\n",
        "      # slide the window over every part of the image using stride s. Take the maximum value at each step.\r\n",
        "      for cuur_channel in range(img_channel):\r\n",
        "        curr_y = out_y = 0\r\n",
        "        # slide the max pooling window vertically across the image\r\n",
        "        while curr_y + self.pool_size <= img_dim:\r\n",
        "            curr_x = out_x = 0\r\n",
        "            # slide the max pooling window horizontally across the image\r\n",
        "            while curr_x + self.pool_size <= img_dim:\r\n",
        "              #obtain the index of the maximum value in that window size\r\n",
        "              a,b = self.get_max_index(self.image[cuur_channel , curr_y:curr_y+self.pool_size, curr_x:curr_x+self.pool_size])\r\n",
        "              dout[cuur_channel , curr_y+a , curr_x+b] = output[cuur_channel , curr_y , curr_x]\r\n",
        "\r\n",
        "              curr_x += self.strides\r\n",
        "              out_x += 1\r\n",
        "            curr_y += self.strides\r\n",
        "            curr_y += 1\r\n",
        "      #print(dout)\r\n",
        "      return dout\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKZCutOkcabM"
      },
      "source": [
        "class Flatten(con2D):\r\n",
        "  \"\"\" get the strignt single channel fully connet layer \"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    self.image = None\r\n",
        "    self.flatten_image = None\r\n",
        "    \r\n",
        "  def forward_propogation(self , image):\r\n",
        "    #store the image  size to use in the backpropogation\r\n",
        "    self.image = image\r\n",
        "    image_channel , input_dim , _ = image.shape\r\n",
        "    a = image.flatten()  #flatten the image \r\n",
        "    self.flatten_image = np.array([a]) #make it to 2D\r\n",
        "    #print(self.flatten_image)\r\n",
        "    #print('flatten image shape for further use',self.flatten_image.shape) \r\n",
        "    return self.flatten_image \r\n",
        "\r\n",
        "  def back_propogation(self , output , learning_rate):\r\n",
        "    output = output.reshape(self.image.shape)\r\n",
        "    #print(output)\r\n",
        "    return output  # mae it into the original size\r\n",
        "    \r\n",
        "      "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ4RKj2mdaGF"
      },
      "source": [
        "class FClayer(Flatten):\r\n",
        "  def __init__(self , input_size , output_size):\r\n",
        "    self.input = None\r\n",
        "    self.input_size = input_size\r\n",
        "    self.output = None\r\n",
        "    # initialize the weights using the normal distribution\r\n",
        "    self.weights = np.random.rand(self.input_size , output_size) - 0.5\r\n",
        "    self.bias =  np.random.rand(1,output_size) - 0.5 \r\n",
        "  \r\n",
        "  def forward_propogation(self , input):\r\n",
        "    self.input = input\r\n",
        "    self.output = np.dot(self.input , self.weights) + self.bias\r\n",
        "    print(self.output)\r\n",
        "    return self.output\r\n",
        "  \r\n",
        "  def back_propogation(self, output_error , learning_rate):\r\n",
        "    #print('coming gradiant' , output_error)\r\n",
        "    #print(' weight' , self.weights)\r\n",
        "    #print('back input ',self.input)\r\n",
        "    #print('bias',self.bias)\r\n",
        "    #print('finding the input error')\r\n",
        "    #print('the weight transpose' , self.weights.T)\r\n",
        "    #print('error',output_error ,'*' ,self.weights.T )\r\n",
        "    input_error = np.dot(output_error , self.weights.T)  #dE/dX = W.T * dE / dY\r\n",
        "    \r\n",
        "    \r\n",
        "    #print('input transpose',self.input.T)\r\n",
        "    weights_error = np.dot(self.input.T , output_error) #dE/dW = X.T * dE / dY\r\n",
        "    bias_error = output_error                           #dE/dB =  dE / dY\r\n",
        "    #update the weights and bias\r\n",
        "    self.weights -= learning_rate *  weights_error\r\n",
        "    self.bias -= learning_rate * bias_error\r\n",
        "    #print(input_error)\r\n",
        "    return input_error\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bQtsIVFXBzv"
      },
      "source": [
        "def softmax_funtion(output):\r\n",
        "  \"\"\" pass the raw predicted data to the activation function to get the probability\"\"\"\r\n",
        "  #output = np.array(output , dtype = np.float64)\r\n",
        "  #print('softmax function',output.dtype)\r\n",
        "  print('coming output',output)\r\n",
        "  out = output - (np.max(output))\r\n",
        "  print('normalized output' , out)\r\n",
        "  out_exp = np.exp(out) # exponent the predicted output\r\n",
        "  print('exponential value',out_exp)\r\n",
        "  return out_exp / np.sum(out_exp)\r\n",
        "\r\n",
        "def softmax_derivated(loss_derivated_error):\r\n",
        "  return loss_derivated_error\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHWBhiT-XT3z"
      },
      "source": [
        "def catogarical_cross_entropy(label , prob):\r\n",
        "  \"\"\" calculate the categorical cross entropy loss\"\"\"\r\n",
        "  return -np.sum(label * np.log(prob)) #multiply the output with the log of probability\r\n",
        "\r\n",
        "def catogarical_cross_entropy_derivated(label , prob):\r\n",
        "  loss_derivated_error =  prob - label \r\n",
        "  print('loss derivated',loss_derivated_error)\r\n",
        "  return loss_derivated_error"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnUTA81OVPh1"
      },
      "source": [
        "class softmax(FClayer):\r\n",
        "  def __init__(self, actvation = softmax_funtion , activation_derivated = softmax_derivated): #default parameter setting\r\n",
        "    self.actvation = actvation #show the activation from the outside class\r\n",
        "    self.activation_derivated = activation_derivated\r\n",
        "\r\n",
        "  def forward_propogation(self , input_data):\r\n",
        "    self.input = input_data \r\n",
        "    self.output = self.actvation(self.input)\r\n",
        "    #print('soft max activation',self.output)\r\n",
        "    return self.output\r\n",
        "\r\n",
        "  def back_propogation(self ,  output_error , learning_rate):\r\n",
        "    return self.activation_derivated(self.input) * output_error"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxpXaBr2ZDgW"
      },
      "source": [
        "class Relu(con2D):\r\n",
        "  \"\"\" pass through the relu activation function\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    self.output = None\r\n",
        "\r\n",
        "  def forward_propogation(self,input):\r\n",
        "    input[input<=0] = 0\r\n",
        "    self.output = input\r\n",
        "    #print(self.output)\r\n",
        "    return self.output\r\n",
        "  \r\n",
        "  def back_propogation(self,output_error , learning_rate):\r\n",
        "    output_error[output_error<=0] = 0\r\n",
        "    return output_error\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7KVBmaF8oC"
      },
      "source": [
        "class network:\r\n",
        "  def __init__(self):\r\n",
        "    self.layers = []\r\n",
        "\r\n",
        "  #add the layer in the model\r\n",
        "  def add(self , layer):\r\n",
        "    self.layers.append(layer)\r\n",
        "  #set the loss function to use it out\r\n",
        "  def setloss(self, loss , loss_derivated):\r\n",
        "    self.loss = loss\r\n",
        "    self.loss_derivated = loss_derivated\r\n",
        "\r\n",
        "  def fit(self , x_train ,  y_train , epoches ,learning_rate):\r\n",
        "    sampels = len(x_train)\r\n",
        "    #loop over for epoches\r\n",
        "    for i in range(epoches):\r\n",
        "      err = 0\r\n",
        "      for j in range(sampels):\r\n",
        "        print('sample' , j)\r\n",
        "        output = x_train[j]  #loop our the training image\r\n",
        "        for layer in self.layers:\r\n",
        "          print(layer)\r\n",
        "          output = layer.forward_propogation(output)\r\n",
        "          \r\n",
        "        print(output.dtype)\r\n",
        "        print('output of softmax',output)\r\n",
        "          \r\n",
        "\r\n",
        "        #compute the loss\r\n",
        "                \r\n",
        "        err += self.loss(y_train[j] , output)\r\n",
        "        print('loss ',err)\r\n",
        "        #backpropogation\r\n",
        "        error = self.loss_derivated(y_train[j] , output)\r\n",
        "        \r\n",
        "        #update the weights\r\n",
        "        print('backporopogation stated')\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "          #print('back',layer)\r\n",
        "          error = layer.back_propogation(error , learning_rate)\r\n",
        "          \r\n",
        "      #displae the error and epoches for the reference\r\n",
        "      err/=sampels\r\n",
        "      print('epoches..'+str(i)+'/'+str(epoches)+'-------'+'Error',err)\r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BIiq9PAqGVz"
      },
      "source": [
        "# Check it with one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI_1_WYMqF9V"
      },
      "source": [
        "#import packages\r\n",
        "import pandas as pd\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB_Rx_O4tara",
        "outputId": "60be31c5-035f-4fea-b107-ea7c765a8a63"
      },
      "source": [
        "#load the image dataset\r\n",
        "(X_train , y_train ), (X_test , y_test )= mnist.load_data()\r\n",
        "print(\"shape of the X_train \", X_train.shape)\r\n",
        "print(\"shape of the X_train \", y_train.shape)\r\n",
        "print(\"shape of the X_train \", X_test.shape)\r\n",
        "print(\"shape of the X_train \", y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "shape of the X_train  (60000, 28, 28)\n",
            "shape of the X_train  (60000,)\n",
            "shape of the X_train  (10000, 28, 28)\n",
            "shape of the X_train  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZTQQ8X15jc_"
      },
      "source": [
        "# Clean the datsset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dgrf3xc5ihD",
        "outputId": "52efd100-cdd3-4dd8-d5a6-517c50e612b6"
      },
      "source": [
        "X_train =   X_train.reshape(X_train.shape[0] , 1 , 28 , 28)\r\n",
        "X_train = X_train.astype('float64')\r\n",
        "X_train /= 255.0\r\n",
        "\r\n",
        "#clean the test dataset\r\n",
        "X_test =   X_test.reshape(X_test.shape[0] , 1 , 28 , 28)\r\n",
        "X_test = X_test.astype('float64')\r\n",
        "X_test /= 255.0\r\n",
        "#print(X_train[0])\r\n",
        "#one hot encode the output\r\n",
        "print(y_train[0])\r\n",
        "y_train = np_utils.to_categorical(y_train)\r\n",
        "print(y_train[0])\r\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "IP-tHuN6qTja",
        "outputId": "74c31e92-3991-49d4-d0a0-e8da1031c39b"
      },
      "source": [
        "#@title Default title text\n",
        "car_path = '/content/drive/MyDrive/Colab Notebooks/dataFiles/DNA_Genome_Analysis_covid19/car.png'\n",
        "car_img = image.load_img(car_path , target_size = (28,28))\n",
        "car_arr = image.img_to_array(car_img)\n",
        "plt.imshow(car_img)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f91ae230f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtEJ1g6OHtBm"
      },
      "source": [
        "#build the model \r\n",
        "model = network()\r\n",
        "model.add(con2D(filter = 1 , kernal_size =(3,3) , strides = (1,1) , img_channel =1))\r\n",
        "model.add(Relu())\r\n",
        "model.add(MaxPooling2D(pool_size = 2))\r\n",
        "model.add(Relu())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(FClayer(625,128))\r\n",
        "model.add(FClayer(128,10))\r\n",
        "model.add(softmax())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MAc1cZ1GMI70",
        "outputId": "a3d50212-4458-4824-c894-581bc0586222"
      },
      "source": [
        "#i = np.array([[[[1,2,1,0,1] , [2,1,1,2,1] , [3,2,1,0,0] , [0,1,2,3,1] , [2,1,1,0,2]]]])\r\n",
        "#print(i.shape)\r\n",
        "#set the loss to compute the loss\r\n",
        "model.setloss(loss = catogarical_cross_entropy , loss_derivated = catogarical_cross_entropy_derivated)\r\n",
        "model.fit(X_train ,y_train ,10 , learning_rate = 0.01)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample 0\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[  1.32269132  -2.00108738   2.11404156   0.99006588  -1.30330136\n",
            "    5.85594929  -2.65066629  -0.62964678  -1.72598633  -1.53493549\n",
            "   -6.44816976  -4.67205516   1.50693884   0.304756    -1.9438659\n",
            "   -3.28841495   4.37446519   4.64881204  -3.93915583   0.2860413\n",
            "    1.42930196   1.7578297   -2.50531888   0.99602639   1.96165054\n",
            "   -4.57832852   0.60271465  -1.19500433   0.82031937   0.89094373\n",
            "   -0.19670669  -5.42094735  -0.82020137  -2.72484382   1.10511145\n",
            "   -1.46727504  -2.2742064   -2.67861606  -4.25379551  -4.70214289\n",
            "   -0.30261479   3.87752596  -1.2592443    1.03512793  -4.22044517\n",
            "   -0.66812461  -5.46476012  -4.70655284  -0.39601454   3.25556198\n",
            "   -1.24439695   3.73284673   1.16161806  -0.04152017   4.47558073\n",
            "   -0.7889336    1.29387329  -0.8713804    0.16058559   1.65434869\n",
            "    0.46384225  -6.29353732   6.71647698  -0.51220889   3.49548332\n",
            "   -1.0818939   -0.68172922  -2.0938253   -0.87182382   4.10984852\n",
            "   -0.71828079   0.77660704   4.24095927   1.20931079  -3.19051322\n",
            "    1.61579773  -0.72170568  -1.43892003  -3.41831001  -0.29431988\n",
            "    1.56423887   1.17576792   2.24040883  -0.09771129  -2.65390891\n",
            "    1.83863609   2.93306609   3.64645484  -1.92210389   2.67651971\n",
            "    1.27566631  -1.40670108  -4.19539239  -2.81512481  -0.22834827\n",
            "    1.06216948   2.62940458   2.71457575  -6.95125351   2.02950622\n",
            "   -2.54345434   2.31100394  -1.71461216  -2.0509841    6.19472741\n",
            "    1.59426351  -4.50567591  -0.5711408    1.51891717   2.77277396\n",
            "   -0.69449324   0.08356546   2.09178732  -1.42773911  -1.46823857\n",
            "    1.31483637   3.73137241   0.13160822   4.46103378   6.43171461\n",
            "    3.08010797   1.22856756  -2.10519308  -0.33463362  -4.8500145\n",
            "  -10.3943959   -4.32471027   4.22467855]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[  1.08148904  21.96910781  -5.37277188  13.27868429 -22.40371497\n",
            "   17.07211866  -3.19731987  -3.53665753   1.18119168  -5.11062304]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[  1.08148904  21.96910781  -5.37277188  13.27868429 -22.40371497\n",
            "   17.07211866  -3.19731987  -3.53665753   1.18119168  -5.11062304]]\n",
            "normalized output [[-20.88761876   0.         -27.34187969  -8.69042351 -44.37282278\n",
            "   -4.89698915 -25.16642767 -25.50576534 -20.78791612 -27.07973085]]\n",
            "exponential value [[8.48442531e-10 1.00000000e+00 1.33528056e-12 1.68188781e-04\n",
            "  5.35954500e-20 7.46903740e-03 1.17587006e-11 8.37503936e-12\n",
            "  9.37395225e-10 1.73549082e-12]]\n",
            "float64\n",
            "output of softmax [[8.42011894e-10 9.92420657e-01 1.32516001e-12 1.66914020e-04\n",
            "  5.31892317e-20 7.41242700e-03 1.16695774e-11 8.31156206e-12\n",
            "  9.30290385e-10 1.72233694e-12]]\n",
            "loss  4.904597362288824\n",
            "loss derivated [[ 8.42011894e-10  9.92420657e-01  1.32516001e-12  1.66914020e-04\n",
            "   5.31892317e-20 -9.92587573e-01  1.16695774e-11  8.31156206e-12\n",
            "   9.30290385e-10  1.72233694e-12]]\n",
            "backporopogation stated\n",
            "sample 1\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[ 18428.51509512  14568.75461592  32111.14411381 -11667.07480099\n",
            "  -20607.70019477 -35601.69968471 -60650.65663997  24851.78850147\n",
            "   71956.59835078  19281.72832471 -74309.72531136  -1397.25270559\n",
            "   13011.19578535 -31896.52091659  15483.83669692 -34871.10325359\n",
            "   47114.35397832 -11706.8752847    3186.75585003  52516.61242493\n",
            "   44958.0256904  -48977.53183136  24735.75330712  72535.66634938\n",
            "    3807.38412089 -15582.90524068  59941.81530299  -4071.74499902\n",
            "  -14471.88789423  43119.04665598 -37043.44673497 -59712.72434707\n",
            "   72077.67828799 -59540.14728673  21860.17324224 -16153.65698195\n",
            "  -20049.94395103 -28926.70552448 -26221.65391525 -66717.72845583\n",
            "  -44907.51027099   4426.10230865  18111.32034643  14166.85591421\n",
            "    6502.59361339 -20516.83359938 -36813.79981153  -3310.34668903\n",
            "    3532.35976112  55638.15554226 -48952.7374638  -18987.5987106\n",
            "    3113.95705209   2318.39811852 -15680.85611731 -51241.26815303\n",
            "  -29394.73112898  -1844.64680696 -17188.58194657  -6046.39901119\n",
            "   85134.61769797 -31724.72456285  56672.95039089   9869.24140204\n",
            "  -27018.62156035  -1941.44616587 -17119.31700401  40832.28987611\n",
            "    9663.11188195  79673.17439061  34412.50879029  56987.95193109\n",
            "    7202.7640232    3868.15816158  21760.8769566  -19581.75911176\n",
            "  -16612.73727981   -644.19753122 -19465.43676564 -12970.75803549\n",
            "  -51744.55247145  57946.87345405  43294.89885986  53349.00963162\n",
            "   -1351.65602347 -26598.82847008   8519.74793154  23367.37711939\n",
            "    7023.48534266  59705.00051836 -45038.64108654 -34681.64416382\n",
            "  -45800.0369701   -5729.03686489 -51297.43121086 -13329.37499134\n",
            "   -8285.77627331  59597.74144904 -49201.09166339 -50693.68027073\n",
            "   -5322.41932011 -71265.4373965  -14351.47219705  51529.98656408\n",
            "   16303.36383335 -42783.70533294 -22569.87650506 -20035.03696794\n",
            "  -16851.34177986  25834.81157166  -6881.15070965  -7232.71010827\n",
            "   12090.12346672  40173.26550567 -11608.30337042  13318.49163006\n",
            "   72860.25917948 -38442.10818813  39387.62670695 -82319.74416563\n",
            "  -16716.16490753  27216.83431541  21501.15604367  23489.80365862\n",
            "   26884.01593591 -45687.20375159 -26864.25136853  -2656.93274356]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[  214799.65225841 -1670843.30650096  -211010.00766243   -39148.2114004\n",
            "    -49708.91596639  1489233.01768969  -174296.56654544   -18169.26716164\n",
            "     56148.86126723   -77111.37533359]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[  214799.65225841 -1670843.30650096  -211010.00766243   -39148.2114004\n",
            "    -49708.91596639  1489233.01768969  -174296.56654544   -18169.26716164\n",
            "     56148.86126723   -77111.37533359]]\n",
            "normalized output [[-1274433.36543128 -3160076.32419065 -1700243.02535212 -1528381.22909009\n",
            "  -1538941.93365609        0.         -1663529.58423513 -1507402.28485133\n",
            "  -1433084.15642246 -1566344.39302328]]\n",
            "exponential value [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "float64\n",
            "output of softmax [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "loss  nan\n",
            "loss derivated [[-1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
            "backporopogation stated\n",
            "sample 2\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[ 1.12657398e+18  1.01011626e+18 -4.01377461e+18  1.19914018e+18\n",
            "  -5.27629979e+17 -3.08250503e+18  4.70787948e+18  8.88271132e+17\n",
            "  -4.21373438e+17  3.43442184e+18  6.16229485e+18  6.20545464e+18\n",
            "  -6.18462248e+17  1.62453974e+18  2.49790326e+18  2.29599241e+18\n",
            "  -5.60263733e+18 -3.66787416e+18  3.50593816e+18 -2.34249514e+18\n",
            "  -7.66227550e+17 -8.22974782e+16  1.04573422e+18 -1.54521920e+18\n",
            "  -2.64831466e+18  5.02035471e+18 -1.31906010e+18  8.57993439e+16\n",
            "  -2.26286993e+17 -3.20948878e+18  4.29455634e+17  6.63573743e+18\n",
            "  -1.41951258e+18  3.31692686e+18 -1.71856994e+18  1.69961597e+18\n",
            "   1.63351088e+18  2.48333897e+18  5.10315811e+18  4.37154998e+18\n",
            "   2.00645763e+18 -9.46855380e+17  7.18497148e+17 -1.19137203e+18\n",
            "   2.03330132e+18  6.94930655e+17  7.12906484e+18  3.98725769e+18\n",
            "   1.30075159e+18 -4.49351461e+18  1.51798335e+18 -2.07833836e+18\n",
            "  -1.87211243e+18  2.30833521e+18 -4.66249254e+18 -1.26682969e+17\n",
            "   1.06559386e+18  2.05141130e+18 -1.08987153e+18 -2.16420349e+18\n",
            "  -2.41554735e+18  7.78337171e+18 -7.63582628e+18  3.00157598e+18\n",
            "  -1.68937211e+18  1.46100837e+18  2.46903647e+18  2.93174917e+18\n",
            "   2.70035285e+18 -5.17908408e+18  1.53764257e+18 -2.35031294e+18\n",
            "  -3.83754049e+18 -3.01596672e+18  1.91133406e+18 -1.76786018e+18\n",
            "   1.75112652e+18  2.23410439e+18  5.43047960e+18  3.84091788e+17\n",
            "   1.10461496e+18 -2.87453273e+18 -2.88132632e+18 -1.18041883e+18\n",
            "   1.60529894e+18 -2.62562768e+17 -3.11866179e+17 -4.43011827e+18\n",
            "   2.02788664e+18 -4.57688297e+18 -6.43998598e+17  1.22986665e+18\n",
            "   2.68054869e+18  3.98513128e+18  1.82371114e+18 -2.57757526e+18\n",
            "  -2.62002077e+18 -2.10634419e+18  7.64633008e+18 -1.04138438e+18\n",
            "   3.28502357e+18  2.53253741e+17 -2.68871077e+17  1.04935228e+18\n",
            "  -4.81127161e+18  1.00206748e+18  4.36757018e+18  9.18961812e+17\n",
            "   2.00765871e+17 -4.60865288e+18  1.93417656e+18 -1.41634040e+18\n",
            "  -3.78646175e+18 -5.12141991e+17  8.05331507e+17 -1.78906010e+18\n",
            "  -3.48000274e+18  1.91706530e+18 -5.39584249e+18 -2.93476064e+18\n",
            "  -2.21435061e+18 -2.88112526e+18  1.69289015e+17 -1.44961080e+18\n",
            "   3.82464391e+18  8.06777783e+18  3.97122593e+18 -3.23777453e+18]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[-1.57243649e+28  2.13443556e+20  1.05230138e+19 -8.68081321e+18\n",
            "   2.30510380e+19  1.09019000e+29  1.02144200e+19  8.56711362e+18\n",
            "  -8.49248895e+17  1.20382415e+18]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[-1.57243649e+28  2.13443556e+20  1.05230138e+19 -8.68081321e+18\n",
            "   2.30510380e+19  1.09019000e+29  1.02144200e+19  8.56711362e+18\n",
            "  -8.49248895e+17  1.20382415e+18]]\n",
            "normalized output [[-1.24743365e+29 -1.09019000e+29 -1.09019000e+29 -1.09019000e+29\n",
            "  -1.09019000e+29  0.00000000e+00 -1.09019000e+29 -1.09019000e+29\n",
            "  -1.09019000e+29 -1.09019000e+29]]\n",
            "exponential value [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "float64\n",
            "output of softmax [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "loss  nan\n",
            "loss derivated [[ 0.  0.  0.  0. -1.  1.  0.  0.  0.  0.]]\n",
            "backporopogation stated\n",
            "sample 3\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[ 2.77247721e+95  2.19179570e+95  4.83095977e+95 -1.75525259e+95\n",
            "  -3.10032462e+95 -5.35609627e+95 -9.12458557e+95  3.73882632e+95\n",
            "   1.08255075e+96  2.90083884e+95 -1.11795236e+96 -2.10209614e+94\n",
            "   1.95746882e+95 -4.79867079e+95  2.32946519e+95 -5.24618171e+95\n",
            "   7.08811706e+95 -1.76124038e+95  4.79431362e+94  7.90085962e+95\n",
            "   6.76370835e+95 -7.36842279e+95  3.72136941e+95  1.09126254e+96\n",
            "   5.72801745e+94 -2.34436954e+95  9.01794397e+95 -6.12573511e+94\n",
            "  -2.17722259e+95  6.48704322e+95 -5.57299984e+95 -8.98347838e+95\n",
            "   1.08437234e+96 -8.95751504e+95  3.28875287e+95 -2.43023627e+95\n",
            "  -3.01641300e+95 -4.35187703e+95 -3.94491565e+95 -1.00373459e+96\n",
            "  -6.75610856e+95  6.65884781e+94  2.72475686e+95  2.13133206e+95\n",
            "   9.78282437e+94 -3.08665420e+95 -5.53845061e+95 -4.98024966e+94\n",
            "   5.31425724e+94  8.37048005e+95 -7.36469260e+95 -2.85658852e+95\n",
            "   4.68479139e+94  3.48791315e+94 -2.35910577e+95 -7.70899051e+95\n",
            "  -4.42228913e+95 -2.77517810e+94 -2.58593552e+95 -9.09650258e+94\n",
            "   1.28080741e+96 -4.77282489e+95  8.52615971e+95  1.48477763e+95\n",
            "  -4.06481543e+95 -2.92080787e+94 -2.57551495e+95  6.14301220e+95\n",
            "   1.45376648e+95  1.19864275e+96  5.17718849e+95  8.57355011e+95\n",
            "   1.08361953e+95  5.81944892e+94  3.27381426e+95 -2.94597696e+95\n",
            "  -2.49930258e+95 -9.69162670e+93 -2.92847683e+95 -1.95138517e+95\n",
            "  -7.78470710e+95  8.71781502e+95  6.51349930e+95  8.02608959e+95\n",
            "  -2.03349833e+94 -4.00165967e+95  1.28175313e+95  3.51550409e+95\n",
            "   1.05664797e+95  8.98231637e+95 -6.77583653e+95 -5.21767855e+95\n",
            "  -6.89038470e+95 -8.61904710e+94 -7.71743996e+95 -2.00533729e+95\n",
            "  -1.24655328e+95  8.96617979e+95 -7.40205623e+95 -7.62660868e+95\n",
            "  -8.00731152e+94 -1.07215258e+96 -2.15910665e+95  7.75242674e+95\n",
            "   2.45275890e+95 -6.43659281e+95 -3.39552415e+95 -3.01417032e+95\n",
            "  -2.53519943e+95  3.88671717e+95 -1.03523444e+95 -1.08812478e+95\n",
            "   1.81889813e+95  6.04386530e+95 -1.74641073e+95  2.00369993e+95\n",
            "   1.09614587e+96 -5.78342140e+95  5.92566989e+95 -1.23845906e+96\n",
            "  -2.51486276e+95  4.09463553e+95  3.23474055e+95  3.53392256e+95\n",
            "   4.04456470e+95 -6.87340950e+95 -4.04159120e+95 -3.99722145e+94]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[ 5.56024614e+105 -2.51369961e+097 -3.17454257e+096 -5.88961847e+095\n",
            "  -2.53867827e+133  1.20065816e+143 -2.62219739e+096 -2.73342380e+095\n",
            "   8.44734348e+095 -1.16009680e+096]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[ 5.56024614e+105 -2.51369961e+097 -3.17454257e+096 -5.88961847e+095\n",
            "  -2.53867827e+133  1.20065816e+143 -2.62219739e+096 -2.73342380e+095\n",
            "   8.44734348e+095 -1.16009680e+096]]\n",
            "normalized output [[-1.20065816e+143 -1.20065816e+143 -1.20065816e+143 -1.20065816e+143\n",
            "  -1.20065816e+143  0.00000000e+000 -1.20065816e+143 -1.20065816e+143\n",
            "  -1.20065816e+143 -1.20065816e+143]]\n",
            "exponential value [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "float64\n",
            "output of softmax [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "loss  nan\n",
            "loss derivated [[ 0. -1.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
            "backporopogation stated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sample 4\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[ inf  inf -inf  inf -inf -inf  inf  inf -inf  inf  inf  inf -inf  inf\n",
            "   inf  inf -inf -inf  inf -inf -inf -inf  inf -inf -inf  inf -inf  inf\n",
            "  -inf -inf  inf  inf -inf  inf -inf  inf  inf  inf  inf  inf  inf -inf\n",
            "   inf -inf  inf  inf  inf  inf  inf -inf  inf -inf -inf  inf -inf -inf\n",
            "   inf  inf -inf -inf -inf  inf -inf  inf -inf  inf  inf  inf  inf -inf\n",
            "   inf -inf -inf -inf  inf -inf  inf  inf  inf  inf  inf -inf -inf -inf\n",
            "   inf -inf -inf -inf  inf -inf -inf  inf  inf  inf  inf -inf -inf -inf\n",
            "   inf -inf  inf  inf -inf  inf -inf  inf  inf  inf  inf -inf  inf -inf\n",
            "  -inf -inf  inf -inf -inf  inf -inf -inf -inf -inf  inf -inf  inf  inf\n",
            "   inf -inf]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[nan nan nan nan inf nan nan nan nan nan]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[nan nan nan nan inf nan nan nan nan nan]]\n",
            "normalized output [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "exponential value [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "float64\n",
            "output of softmax [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "loss  nan\n",
            "loss derivated [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "backporopogation stated\n",
            "sample 5\n",
            "<__main__.con2D object at 0x7f91a51e0210>\n",
            "<__main__.Relu object at 0x7f91a51e0190>\n",
            "<__main__.MaxPooling2D object at 0x7f91a5205610>\n",
            "<__main__.Relu object at 0x7f91a524d6d0>\n",
            "<__main__.Flatten object at 0x7f91a524d090>\n",
            "<__main__.FClayer object at 0x7f91a524d110>\n",
            "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan]]\n",
            "<__main__.FClayer object at 0x7f91a524d1d0>\n",
            "[[nan nan nan nan nan nan nan nan nan nan]]\n",
            "<__main__.softmax object at 0x7f91a534ba10>\n",
            "coming output [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "normalized output [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "exponential value [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "float64\n",
            "output of softmax [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "loss  nan\n",
            "loss derivated [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "backporopogation stated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-94fc7b35e29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#set the loss to compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatogarical_cross_entropy\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss_derivated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatogarical_cross_entropy_derivated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-3c2e62876cde>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epoches, learning_rate)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;31m#print('back',layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propogation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m#displae the error and epoches for the reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e354e226bd24>\u001b[0m in \u001b[0;36mback_propogation\u001b[0;34m(self, output, learning_rate)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mimg_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0;31m#obtain the index of the maximum value in that window size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m               \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcuur_channel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurr_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurr_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m               \u001b[0mdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcuur_channel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcuur_channel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_y\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e354e226bd24>\u001b[0m in \u001b[0;36mget_max_index\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_max_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;34m\"\"\"return the index of the maximum value in the array\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mflat_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanargmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0midex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_index\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnanargmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36mnanargmax\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All-NaN slice encountered\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All-NaN slice encountered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpvgvIc-kQrb"
      },
      "source": [
        "s = np.array([[[1,0,1] , [2,1,1] , [0,1,2]]])\r\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPCfdrQShnAK"
      },
      "source": [
        "n = np.array([[ 1889.1599303 ,   309.33289731 ,  152.73517451 ,-1600.86077456,\r\n",
        "   2505.15245823  , 749.31314978 , 1166.17901765 ,-4494.43938188,\r\n",
        "   1976.19386057, -2421.32088513]])\r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(n)):\r\n",
        "  n[i] = np.exp(n[i]) \r\n",
        "print(n)\r\n",
        "sum = np.sum(n) \r\n",
        "print(sum) \r\n",
        "for i in range(len(n)):\r\n",
        "  print(n[i])\r\n",
        "  n[i] = (n[i] / sum )\r\n",
        "print(sum , n)\r\n",
        "\r\n",
        "np.sum(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdmN0b7pAe7u"
      },
      "source": [
        "301.95428447325344/520.2942363539252"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmjpskyVA4Mh"
      },
      "source": [
        "np.exp(5270.61915528)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgGtMYLSM2GU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}